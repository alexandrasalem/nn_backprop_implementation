{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex Salem\n",
    "# CS 559: Machine Learning\n",
    "# Final Project: Implementation of Multilayer Feed-Forward Neural Network with Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,\n",
    "            edgecolor='k')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Iris Dataset: Sepal length and width for the three types\")\n",
    "plt.savefig(\"iris_data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single hidden layer, any number of output nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are helper functions I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_bias_term(matrix):\n",
    "    new_col = np.zeros([len(matrix),1])\n",
    "    new_col+=1\n",
    "    return np.hstack((new_col, matrix))\n",
    "\n",
    "def sigmoid(x_vector):\n",
    "    res = 1/(1+np.exp(-x_vector))\n",
    "    return res\n",
    "\n",
    "def predict_class(output_vector):\n",
    "    class_outputs = []\n",
    "    for elt in output_vector:\n",
    "        elt = list(elt)\n",
    "        class_outputs.append(elt.index(max(elt)))\n",
    "    return class_outputs\n",
    "\n",
    "def standardize_matrix_x_data(mat_x):\n",
    "    copy = mat_x\n",
    "    list_of_cols = []\n",
    "    for col in copy.T:\n",
    "        x_min = np.min(col)\n",
    "        x_max = np.max(col)\n",
    "        new_col = (col-x_min)/(x_max-x_min)\n",
    "        list_of_cols.append(new_col)\n",
    "    new_mat = np.array(np.array(list_of_cols))\n",
    "    new_mat = new_mat.T\n",
    "    return new_mat\n",
    "\n",
    "def initialize_network(X, y, num_hidden, num_hidden_layers):\n",
    "    hidden_layers_weights = []\n",
    "    for i in range(num_hidden_layers):\n",
    "        if len(hidden_layers_weights)==0:\n",
    "            hidden_weights = 2*(np.random.random((X.shape[1] + 1, num_hidden)))-1\n",
    "            hidden_layers_weights.append(hidden_weights)\n",
    "        else:\n",
    "            hidden_weights = 2*(np.random.random((num_hidden+1, num_hidden)))-1\n",
    "            hidden_layers_weights.append(hidden_weights)\n",
    "    output_weights = 2*(np.random.random((num_hidden + 1, y.shape[1])))-1\n",
    "    return hidden_layers_weights, output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 1-layer implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# learning rate\n",
    "alpha = .1\n",
    "\n",
    "# number of nodes in the hidden layer\n",
    "num_hidden = 3\n",
    "\n",
    "# inputs\n",
    "X = iris.data#[:, :2]  # we only take the first two features.\n",
    "X = standardize_matrix_x_data(X)\n",
    "iris_target_copy = iris.target\n",
    "if iris_target_copy[0] == 0:\n",
    "    new_iris_target = np.array([[1,0,0]])\n",
    "elif iris_target_copy[0] == 1:\n",
    "    new_iris_target = np.array([[0,1,0]])\n",
    "else:\n",
    "    new_iris_target = np.array([[0,0,1]])\n",
    "for i in range(1, len(iris_target_copy)):\n",
    "    if iris_target_copy[i] == 0:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[1,0,0]]), axis=0)\n",
    "    elif iris_target_copy[i] == 1:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[0,1,0]]), axis=0)\n",
    "    else:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[0,0,1]]), axis=0)\n",
    "#y = np.array([iris.target]).T\n",
    "y = new_iris_target\n",
    "X_with_bias = append_bias_term(X)\n",
    "\n",
    "hidden_weights = 2*(np.random.random((X.shape[1] + 1, num_hidden)))-1\n",
    "output_weights = 2*(np.random.random((num_hidden + 1, y.shape[1])))-1\n",
    "print(\"hidden weights\")\n",
    "print(hidden_weights)\n",
    "print(\"output_weights\")\n",
    "print(output_weights)\n",
    "\n",
    "num_iterations = 10000\n",
    "\n",
    "for i in range (0, num_iterations):\n",
    "    #forward pass\n",
    "    #X with bias times hidden initial\n",
    "    #then sigmoid\n",
    "    input_matrix = X_with_bias\n",
    "    hidden_output = append_bias_term(sigmoid(np.matmul(input_matrix, hidden_weights)))\n",
    "    output_output = sigmoid(np.matmul(hidden_output, output_weights))\n",
    "    \n",
    "    #backward pass\n",
    "    output_error_delta_1_m = (output_output) *(1-output_output)* (output_output - y) \n",
    "    hidden_error_delta_j_k = (hidden_output[:, 1:] * (1.0-hidden_output[:, 1:])) * np.matmul(output_error_delta_1_m, output_weights.T[:,1:])\n",
    "    \n",
    "    #calculate partial derivatives\n",
    "    #output_error_pd = np.matmul(hidden_output.T, output_error_delta_1_m)\n",
    "    #hidden_error_pd = np.matmul(output_output.T, hidden_error_delta_j_k)\n",
    "    \n",
    "    hidden_pd = input_matrix[:, :, np.newaxis] * hidden_error_delta_j_k[: , np.newaxis, :]\n",
    "    output_pd = hidden_output[:, :, np.newaxis] * output_error_delta_1_m[:, np.newaxis, :]\n",
    "    #hidden_pds = []\n",
    "    #output_pds = []\n",
    "    \n",
    "    #for i in range(0,len(X)):\n",
    "    #    hidden_pds.append(np.matmul(input_matrix[i][:,np.newaxis], hidden_error_delta_j_k[i][np.newaxis, :]))\n",
    "    #    output_pds.append(np.matmul(hidden_output[i][:,np.newaxis], output_error_delta_1_m[i][np.newaxis, :]))\n",
    "        \n",
    "    #print(hidden_pds)\n",
    "    #print(output_pds)\n",
    "    \n",
    "    #hidden_pd_average = hidden_pds[0]\n",
    "    #for x in hidden_pds[1:]:\n",
    "    #    hidden_pd_average+=x\n",
    "    #hidden_pd_average = hidden_pd_average/len(hidden_pds) \n",
    "    \n",
    "    #output_pd_average = output_pds[0]\n",
    "    #for x in output_pds[1:]:\n",
    "    #    output_pd_average+=x\n",
    "    #output_pd_average = output_pd_average/len(output_pds)\n",
    "    \n",
    "    hidden_pd_average = np.average(hidden_pd, axis=0)\n",
    "    output_pd_average = np.average(output_pd, axis=0)\n",
    "\n",
    "    #print(output_pd_average)\n",
    "    #update weights\n",
    "    hidden_weights += - alpha * hidden_pd_average\n",
    "    output_weights += - alpha * output_pd_average\n",
    "    \n",
    "#print(output_output)\n",
    "\n",
    "#next need to generalize for many hidden layers and many ouputs\n",
    "#need to limit to two classes\n",
    "\n",
    "predict_class(output_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Xor problem with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [-1, 1],\n",
    "    [1, -1],\n",
    "    [-1, -1],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 1, 1])\n",
    "clf = LogisticRegression(solver='liblinear', \n",
    "                         max_iter=800).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing 2-layer by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW WORKS DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data and seeds\n",
    "# choose a random seed for reproducible results\n",
    "np.random.seed(1)\n",
    "\n",
    "# learning rate\n",
    "alpha = .1\n",
    "\n",
    "# number of nodes in the hidden layer\n",
    "num_hidden = 3\n",
    "\n",
    "X = iris.data\n",
    "X = standardize_matrix_x_data(X)\n",
    "iris_target_copy = iris.target\n",
    "if iris_target_copy[0] == 0:\n",
    "    new_iris_target = np.array([[1,0,0]])\n",
    "elif iris_target_copy[0] == 1:\n",
    "    new_iris_target = np.array([[0,1,0]])\n",
    "else:\n",
    "    new_iris_target = np.array([[0,0,1]])\n",
    "for i in range(1, len(iris_target_copy)):\n",
    "    if iris_target_copy[i] == 0:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[1,0,0]]), axis=0)\n",
    "    elif iris_target_copy[i] == 1:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[0,1,0]]), axis=0)\n",
    "    else:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[0,0,1]]), axis=0)\n",
    "#y = np.array([iris.target]).T\n",
    "y = new_iris_target\n",
    "X_with_bias = append_bias_term(X)\n",
    "\n",
    "\n",
    "hidden_weights_0 = 2*(np.random.random((X.shape[1] + 1, num_hidden)))-1\n",
    "hidden_weights_1 = 2*(np.random.random((num_hidden + 1, num_hidden)))-1\n",
    "output_weights = 2*(np.random.random((num_hidden + 1, y.shape[1])))-1\n",
    "\n",
    "num_iterations = 10000\n",
    "\n",
    "for i in range (0, num_iterations):\n",
    "    #forward pass\n",
    "    input_matrix = X_with_bias\n",
    "    hidden_output_0 = append_bias_term(sigmoid(np.matmul(input_matrix, hidden_weights_0)))\n",
    "    hidden_output_1 = append_bias_term(sigmoid(np.matmul(hidden_output_0, hidden_weights_1)))\n",
    "    output_output = sigmoid(np.matmul(hidden_output_1, output_weights))\n",
    "\n",
    "    #backward pass\n",
    "    output_error_delta_1_m = (output_output) *(1-output_output)* (output_output - y)\n",
    "    hidden_error_delta_1_2 = (hidden_output_1[:, 1:] * (1.0-hidden_output_1[:, 1:])) * np.matmul(output_error_delta_1_m, output_weights.T[:,1:])\n",
    "    hidden_error_delta_0_1 = (hidden_output_0[:, 1:] * (1.0-hidden_output_0[:, 1:])) * np.matmul(hidden_error_delta_1_2, hidden_weights_1.T[:,1:])\n",
    "    \n",
    "    #calculate partial derivatives\n",
    "    hidden_pd_0 = input_matrix[:, :, np.newaxis] * hidden_error_delta_0_1[: , np.newaxis, :]\n",
    "    hidden_pd_1 = hidden_output_0[:, :, np.newaxis] * hidden_error_delta_1_2[:, np.newaxis, :]\n",
    "    output_pd = hidden_output_1[:, :, np.newaxis] * output_error_delta_1_m[:, np.newaxis, :]\n",
    "    \n",
    "    hidden_pd_0_average = np.average(hidden_pd_0, axis=0)\n",
    "    hidden_pd_1_average = np.average(hidden_pd_1, axis=0)\n",
    "    output_pd_average = np.average(output_pd, axis=0)\n",
    "\n",
    "    #print(output_pd_average)\n",
    "    #update weights\n",
    "    hidden_weights_0 += - alpha * hidden_pd_0_average\n",
    "    hidden_weights_1 += - alpha * hidden_pd_1_average\n",
    "    output_weights += - alpha * output_pd_average\n",
    "    \n",
    "#print(output_output)\n",
    "\n",
    "#next need to generalize for many hidden layers and many ouputs\n",
    "#need to limit to two classes\n",
    "\n",
    "predict_class(output_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized and Final Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net(X_train_data, X_test_data, y_train_data, y_test_data, num_iterations, hidden_weights, output_weights, alpha):\n",
    "    MSE_train = []\n",
    "    MSE_test = []\n",
    "    for i in range (0, num_iterations):\n",
    "        #print(i)\n",
    "        if i!=0 and float(i)% 100 == 0.0:\n",
    "            #print(i)\n",
    "            MSE_train_ex = mean_squared_error(y_train_data, output_output)\n",
    "            #print(MSE_train_ex)\n",
    "            MSE_train.append(MSE_train_ex)\n",
    "            input_matrix = append_bias_term(X_test_data)\n",
    "            hidden_outputs = []\n",
    "            for i in range(0, len(hidden_weights)):\n",
    "                if len(hidden_outputs) == 0:\n",
    "                    prev = input_matrix\n",
    "                else:\n",
    "                    prev = hidden_outputs[-1]\n",
    "                hidden_output = append_bias_term(sigmoid(np.matmul(prev, hidden_weights[i])))\n",
    "                hidden_outputs.append(hidden_output)\n",
    "            output_output = sigmoid(np.matmul(hidden_outputs[-1], output_weights))\n",
    "            MSE_test_ex = mean_squared_error(y_test_data, output_output)\n",
    "            MSE_test.append(MSE_test_ex)\n",
    "        #forward pass general\n",
    "        input_matrix = append_bias_term(X_train_data)\n",
    "        hidden_outputs = []\n",
    "        for i in range(0, len(hidden_weights)):\n",
    "            if len(hidden_outputs) == 0:\n",
    "                prev = input_matrix\n",
    "            else:\n",
    "                prev = hidden_outputs[-1]\n",
    "            hidden_output = append_bias_term(sigmoid(np.matmul(prev, hidden_weights[i])))\n",
    "            hidden_outputs.append(hidden_output)\n",
    "        output_output = sigmoid(np.matmul(hidden_outputs[-1], output_weights))\n",
    "\n",
    "        #backward pass general\n",
    "        output_error_delta_1_m = (output_output) *(1-output_output)* (output_output - y_train_data)\n",
    "        hidden_error_deltas = []\n",
    "        for i in list(reversed(range(0, len(hidden_outputs)))):\n",
    "            hidden_output_now = hidden_outputs[i]\n",
    "            if i == len(hidden_outputs)-1:\n",
    "                prev_error = output_error_delta_1_m\n",
    "                prev_weights = output_weights\n",
    "                hidden_error_delta_j_k = (hidden_output_now[:, 1:] * (1.0-hidden_output_now[:, 1:])) * np.matmul(prev_error, prev_weights[1:, :].T)\n",
    "            else:\n",
    "                prev_error = hidden_error_deltas[-1]\n",
    "                prev_weights = hidden_weights[i+1]\n",
    "                hidden_error_delta_j_k = (hidden_output_now[:, 1:] * (1.0-hidden_output_now[:, 1:])) * np.matmul(prev_error, prev_weights[1:,:].T)        \n",
    "            hidden_error_deltas.append(hidden_error_delta_j_k)\n",
    "\n",
    "        hidden_error_deltas_correct_order = list(reversed(hidden_error_deltas))\n",
    "\n",
    "        #calculate partial derivatives general\n",
    "        hidden_pds = []\n",
    "        hidden_pd_now = input_matrix[:, :, np.newaxis] * hidden_error_deltas_correct_order[0][: , np.newaxis, :]\n",
    "        hidden_pds.append(hidden_pd_now)\n",
    "        for i in range(0, len(hidden_outputs)-1):\n",
    "            prev_output = hidden_outputs[i][:, :, np.newaxis]\n",
    "            prev_error = hidden_error_deltas_correct_order[i+1][: , np.newaxis, :]\n",
    "            hidden_pd_now = prev_output * prev_error\n",
    "            hidden_pds.append(hidden_pd_now)\n",
    "        output_pd = hidden_outputs[-1][:, :, np.newaxis] * output_error_delta_1_m[:, np.newaxis, :]\n",
    "\n",
    "        #average partial derivatives:\n",
    "        average_hidden_pds = []\n",
    "        for hidden_pd_now in hidden_pds:\n",
    "            average_hidden_pd = np.average(hidden_pd_now, axis=0)\n",
    "            average_hidden_pds.append(average_hidden_pd)\n",
    "        average_output_pd = np.average(output_pd, axis = 0)\n",
    "\n",
    "        #update weights general\n",
    "        for i in range(len(hidden_weights)):\n",
    "            hidden_weights[i] += - alpha * average_hidden_pds[i]\n",
    "        output_weights += - alpha * average_output_pd\n",
    "\n",
    "    train_output = output_output\n",
    "    #forward pass general\n",
    "    input_matrix = append_bias_term(X_test_data)\n",
    "    hidden_outputs = []\n",
    "    for i in range(0, len(hidden_weights)):\n",
    "        if len(hidden_outputs) == 0:\n",
    "            prev = input_matrix\n",
    "        else:\n",
    "            prev = hidden_outputs[-1]\n",
    "        hidden_output = append_bias_term(sigmoid(np.matmul(prev, hidden_weights[i])))\n",
    "        hidden_outputs.append(hidden_output)\n",
    "    output_output = sigmoid(np.matmul(hidden_outputs[-1], output_weights))\n",
    "    test_output = output_output\n",
    "                                                                      \n",
    "    return train_output, test_output, MSE_train, MSE_test\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "new_X_train = standardize_matrix_x_data(X_train)\n",
    "new_X_test = standardize_matrix_x_data(X_test)\n",
    "\n",
    "iris_target_copy = y_train\n",
    "if iris_target_copy[0] == 0:\n",
    "    new_iris_target = np.array([[1,0,0]])\n",
    "elif iris_target_copy[0] == 1:\n",
    "    new_iris_target = np.array([[0,1,0]])\n",
    "else:\n",
    "    new_iris_target = np.array([[0,0,1]])\n",
    "for i in range(1, len(iris_target_copy)):\n",
    "    if iris_target_copy[i] == 0:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[1,0,0]]), axis=0)\n",
    "    elif iris_target_copy[i] == 1:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[0,1,0]]), axis=0)\n",
    "    else:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[0,0,1]]), axis=0)\n",
    "new_y_train = new_iris_target\n",
    "\n",
    "\n",
    "iris_target_copy = y_test\n",
    "if iris_target_copy[0] == 0:\n",
    "    new_iris_target = np.array([[1,0,0]])\n",
    "elif iris_target_copy[0] == 1:\n",
    "    new_iris_target = np.array([[0,1,0]])\n",
    "else:\n",
    "    new_iris_target = np.array([[0,0,1]])\n",
    "for i in range(1, len(iris_target_copy)):\n",
    "    if iris_target_copy[i] == 0:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[1,0,0]]), axis=0)\n",
    "    elif iris_target_copy[i] == 1:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[0,1,0]]), axis=0)\n",
    "    else:\n",
    "        new_iris_target = np.append(new_iris_target, np.array([[0,0,1]]), axis=0)\n",
    "#y = np.array([iris.target]).T\n",
    "new_y_test = new_iris_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "hidden_weights, output_weights = initialize_network(new_X_train, new_y_train, 3, 1)\n",
    "output_train, output_test, MSE_train_res, MSE_test_res = run_net(new_X_train, new_X_test, new_y_train, new_y_test, 6000, hidden_weights, output_weights, .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MSE_train_res, 'ro', label = \"MSE train\")\n",
    "plt.plot(MSE_test_res, 'bx', label = \"MSE test\")\n",
    "plt.xlabel(\"Iterations/100\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE for 2-layer NN on Iris Data\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"2_layer_iris.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predicted = predict_class(output_train)\n",
    "y_test_predicted = predict_class(output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for 2-layer NN, training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train_predicted, \n",
    "                 list(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for 2-layer NN, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_predicted, \n",
    "                 list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [-1, 1],\n",
    "    [1, -1],\n",
    "    [-1, -1],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([[1,0], \n",
    "              [1,0], \n",
    "              [0,1],\n",
    "              [0,1]])\n",
    "\n",
    "hidden_weights, output_weights = initialize_network(X, y, 3, 2)\n",
    "output_train, output_test, MSE_train_res, MSE_test_res = run_net(X, y, X, y, \n",
    "                                                                 10000, hidden_weights, output_weights, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [-1, 1],\n",
    "    [1, -1],\n",
    "    [-1, -1],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 1, 1])\n",
    "clf = LogisticRegression(random_state=0, \n",
    "                         solver=\"lbfgs\").fit(X, y)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "new_X_train = standardize_matrix_x_data(X_train)\n",
    "new_X_test = standardize_matrix_x_data(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver=\"lbfgs\", \n",
    "                         multi_class='auto', max_iter=200).fit(X_train, y_train)\n",
    "y_test_predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_predicted, list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
